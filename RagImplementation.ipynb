{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf-gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import uuid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file = \"./key.txt\") as f:\n",
    "    api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Loading our trained Binary and multiclass classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classifier(tweet, model_path=\"./models/binary_model\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #tokenize input\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    label_map = {1: \"non-informative\", 0: \"informative\"}\n",
    "    return label_map[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanitarianClassifier(tweet, model_path=\"./models/multi-class-humanitarian_model\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    #tokenize input\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    label_map = {0:'affected_individuals', 1: 'infrastructure_and_utility_damage', 2: 'injured_or_dead_people', 3:'missing_or_found_people', 4: 'not_humanitarian', 5:'other_relevant_information',6:'rescue_volunteering_or_donation_effort', 7:'vehicle_damage'}\n",
    "    return label_map[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_tweet(tweet):\n",
    "    info_category = binary_classifier(tweet)\n",
    "    if info_category==\"non-informative\":\n",
    "        return \"This tweet doesn't contain any disaster related information.\"\n",
    "    humanitarian_category = humanitarianClassifier(tweet)\n",
    "    return info_category, humanitarian_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_gpt_extractor(tweet, existing_summary=None):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in extracting structured information from tweets about disasters.\n",
    "    Given the tweet and existing event summary, return a JSON with important details about the event.\n",
    "    The JSON may include the following fields:\n",
    "    - event_type\n",
    "    - location\n",
    "    - people_killed\n",
    "    - people_trapped\n",
    "    - infrastructure_damage\n",
    "    - any other details you find relevant\n",
    "    - summary (updated, more detailed)\n",
    "\n",
    "    Tweet: \"{tweet}\"\n",
    "    Existing Event Summary: \"{existing_summary or 'None'}\"\n",
    "\n",
    "    Respond only in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informative injured_or_dead_people\n"
     ]
    }
   ],
   "source": [
    "string = \"There were 19 killed in india and pakistan fight; chance of nuclear fight was positive\"\n",
    "info, humanitarian = categorize_tweet(string)\n",
    "print(info, humanitarian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = call_gpt_extractor(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Implementing Fiass vectorDB and json storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 384\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "json_storage_path = \"./event_store/event_metadata.json\"\n",
    "id_to_metadata= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (735953769.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 59\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class RAGBackend:\n",
    "    def __init__(self, json_path=\"./event_store/event_metadata.json\", index_path= \"./event_store/faiss_index.index\",mapping_path=\"id_mapping.json\"):\n",
    "        self.json_path = json_path\n",
    "        self.index_path = index_path\n",
    "        self.mapping_path = mapping_path\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.dim = self.model.getSentenceEmbeddingDim()\n",
    "        self.documents = self.load_or_create_json_store()\n",
    "        self.index = self.load_or_create_faiss_index()\n",
    "        self.id_mapping = self.load_or_create_id_mapping()\n",
    "\n",
    "\n",
    "    def load_or_create_json_store(self):\n",
    "        if os.path.exists(self.json_path):\n",
    "            with open(self.json_path,\"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            with open(self.json_path, \"w\") as f:\n",
    "                json.dump([],f)\n",
    "            return []\n",
    "        \n",
    "    def load_or_create_faiss_index(self):\n",
    "        if os.path.exists(self.index_path):\n",
    "            return faiss.read_index(self.index_path)\n",
    "        else:\n",
    "            return faiss.IndexFlatL2(self.dim)\n",
    "        \n",
    "    def load_or_create_id_mapping(self):\n",
    "        if os.path.exists(self.mapping_path):\n",
    "            with open(self.mapping_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "        \n",
    "    def save_all(self):\n",
    "        with open(self.json_path, \"w\") as f:\n",
    "            json.dump(self.documents, f, indent=2)\n",
    "        faiss.write_index(self.index, self.index_path)\n",
    "        with open(self.mapping_path, \"w\") as f:\n",
    "            json.dump(self.id_mapping, f, indent=2)\n",
    "    \n",
    "    def add_documents(self, text):\n",
    "        doc_id = str(uuid())\n",
    "        new_doc = {\"id\":doc_id, \"text\": text}\n",
    "        self.documents.append(new_doc)\n",
    "\n",
    "        #adding to faiss\n",
    "        vector = self.model.encode([text])\n",
    "        self.index.add(vector)\n",
    "\n",
    "        self.id_mapping[str(len(self.id_mapping))] = doc_id\n",
    "        self.save_all()\n",
    "\n",
    "        return doc_id\n",
    "    \n",
    "    def search_documents(self, query, top_k=3):\n",
    "        query_vector = self.model.encode([query])\n",
    "        distances, indices = self.index.search(query_vector, top_k)\n",
    "        2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Chroma since it allows to update and delete individual docs which faiss doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement uuid4 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for uuid4\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from uuid import uuid4\n",
    "class RAGBackend:\n",
    "    def __init__(self, json_path=\"events.json\", db_path=\"./chroma_db\", collection_name=\"events\"):\n",
    "        self.json_path = json_path\n",
    "        self.db_path = db_path\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.similarity_threshold = 0.4  # Adjust as needed\n",
    "\n",
    "        # Loading document store\n",
    "        self.documents = self.load_or_create_json_store()\n",
    "\n",
    "        # Initializing Chroma DB\n",
    "        client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        self.collection = client.get_or_create_collection(\"events\")\n",
    "\n",
    "    def load_or_create_json_store(self):\n",
    "        if os.path.exists(self.json_path):\n",
    "            with open(self.json_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            with open(self.json_path, \"w\") as f:\n",
    "                json.dump([],f)\n",
    "            return []\n",
    "        \n",
    "    def save_json_stor(self):\n",
    "        with open(self.json_path, \"w\") as f:\n",
    "            json.dump(self.documents, f, indent=2)\n",
    "\n",
    "    def add_documents(self, text):\n",
    "        doc_id = str(uuid4())\n",
    "        doc = {\"id\": doc_id, \"text\":text}\n",
    "        self.documents.append(doc)\n",
    "        self.save_json_stor()\n",
    "        #saving in chroma\n",
    "        self.collection.add(documents=[text], ids=[doc_id])\n",
    "        return doc_id\n",
    "    \n",
    "    def update_document(self, doc_id, new_text):\n",
    "        found = False\n",
    "        for doc in self.documents:\n",
    "            if doc[\"id\"] == doc_id:\n",
    "                doc[\"text\"] = new_text\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            return False\n",
    "        \n",
    "        self.save_json_stor()\n",
    "        self.collection.update(ids=[doc_id], documents=[new_text])\n",
    "        return True\n",
    "    \n",
    "    def delete_document(self, doc_id):\n",
    "        before = len(self.documents)\n",
    "        self.documents = [doc for doc in self.documents if doc[\"id\"]!=doc_id]\n",
    "        after = len(self.documents)\n",
    "        if before == after:\n",
    "            return False\n",
    "        \n",
    "        self.save_json_store()\n",
    "        self.collection.delete(ids=[doc_id])\n",
    "        return True\n",
    "    \n",
    "    def search_similar_event(self, text, top_k=1):\n",
    "        embedding = self.model.encode(text)\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=top_k)\n",
    "        ids = results.get(\"ids\",[[]])[0]\n",
    "        distances = results.get(\"distances\",[[]])[0]\n",
    "        if ids and distances:\n",
    "            return {\"id\": ids[0], \"distance\": distances[0]}\n",
    "        return None\n",
    "    \n",
    "    def process_event(self, text):\n",
    "        result = self.search_similar_event(text, top_k=1)\n",
    "        print(\"result found: \", result)\n",
    "        if result and result[\"distance\"]< self.similarity_threshold:\n",
    "            self.update_document(result[\"id\"], text)\n",
    "            return {\"action\": \"updated\", \"id\": result[\"id\"], \"distance\": result[\"distance\"]}\n",
    "        else:\n",
    "            new_id = self.add_documents(text)\n",
    "            return {\"action\": \"added\", \"id\": new_id}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_extractor(tweet, existing_summary=None):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in extracting structured information from tweets about disasters.\n",
    "    Given the tweet and existing event summary, return a JSON with important details about the event.\n",
    "    The JSON may include the following fields:\n",
    "    - event_type\n",
    "    - location\n",
    "    - people_killed\n",
    "    - people_trapped\n",
    "    - infrastructure_damage\n",
    "    - any other details you find relevant\n",
    "    - summary (updated, more detailed)\n",
    "\n",
    "    Tweet: \"{tweet}\"\n",
    "    Existing Event Summary: \"{existing_summary or 'None'}\"\n",
    "\n",
    "    Respond only in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from uuid import uuid4\n",
    "class RAGBackend1:\n",
    "    def __init__(self, json_path=\"events.json\", db_path=\"./chroma_db\", collection_name=\"events\"):\n",
    "        self.json_path = json_path\n",
    "        self.db_path = db_path\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.similarity_threshold = 0.4  # Adjust as needed\n",
    "\n",
    "        # Loading document store\n",
    "        self.documents = self.load_or_create_json_store()\n",
    "\n",
    "        # Initializing Chroma DB\n",
    "        client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        self.collection = client.get_or_create_collection(\"events\")\n",
    "\n",
    "    def load_or_create_json_store(self):\n",
    "        if os.path.exists(self.json_path):\n",
    "            with open(self.json_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            with open(self.json_path, \"w\") as f:\n",
    "                json.dump([],f)\n",
    "            return []\n",
    "        \n",
    "    def save_json_store(self):\n",
    "        with open(self.json_path, \"w\") as f:\n",
    "            json.dump(self.documents, f, indent=2)\n",
    "\n",
    "    def add_document(self, event: dict):\n",
    "        doc_id = str(uuid4())\n",
    "        event[\"id\"] = doc_id\n",
    "\n",
    "        self.documents.append(event)\n",
    "        self.save_json_store()\n",
    "\n",
    "        summary = event.get(\"summary\", \"\")\n",
    "        self.collection.add(documents=[summary], ids=[doc_id])\n",
    "        return doc_id\n",
    "\n",
    "        \n",
    "    \n",
    "    def update_document(self, doc_id, new_event: dict):\n",
    "        found = False\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            if doc[\"id\"] == doc_id:\n",
    "                self.documents[i] = {**new_event, \"id\": doc_id}\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            return False\n",
    "\n",
    "        self.save_json_store()\n",
    "\n",
    "        summary = new_event.get(\"summary\", \"\")\n",
    "        self.collection.update(ids=[doc_id], documents=[summary])\n",
    "        return True\n",
    "    \n",
    "    def delete_document(self, doc_id):\n",
    "        before = len(self.documents)\n",
    "        self.documents = [doc for doc in self.documents if doc[\"id\"]!=doc_id]\n",
    "        after = len(self.documents)\n",
    "        if before == after:\n",
    "            return False\n",
    "        \n",
    "        self.save_json_store()\n",
    "        self.collection.delete(ids=[doc_id])\n",
    "        return True\n",
    "    \n",
    "    def search_similar_event(self, summary, top_k=1):\n",
    "        embedding = self.model.encode(summary)\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=top_k)\n",
    "        ids = results.get(\"ids\", [[]])[0]\n",
    "        distances = results.get(\"distances\", [[]])[0]\n",
    "        if ids and distances:\n",
    "            return {\"id\": ids[0], \"distance\": distances[0]}\n",
    "        return None\n",
    "    \n",
    "\n",
    "        \n",
    "    def process_event(self, event: dict):\n",
    "        if isinstance(event, str):\n",
    "            try:\n",
    "                event = json.loads(event)\n",
    "                print(type(event))\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON string passed to process_event\")\n",
    "        summary = event.get(\"summary\", \"\")\n",
    "        if not summary:\n",
    "            raise ValueError(\"Event must include a summary field\")\n",
    "\n",
    "        result = self.search_similar_event(summary, top_k=1)\n",
    "        print(\"result found:\", result)\n",
    "\n",
    "        if result and result[\"distance\"] < self.similarity_threshold:\n",
    "            self.update_document(result[\"id\"], event)\n",
    "            return {\"action\": \"updated\", \"id\": result[\"id\"], \"distance\": result[\"distance\"]}\n",
    "        else:\n",
    "            new_id = self.add_document(event)\n",
    "            return {\"action\": \"added\", \"id\": new_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_gpt_extractor(tweet, existing_summary=None):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in extracting structured information from tweets about disasters.\n",
    "    Given the tweet and existing event summary, return a JSON with important details about the event.\n",
    "    The JSON may include the following fields:\n",
    "    - event_type\n",
    "    - location\n",
    "    - people_killed\n",
    "    - people_trapped\n",
    "    - infrastructure_damage\n",
    "    - any other details you find relevant\n",
    "    - summary (updated, more detailed)\n",
    "\n",
    "    Tweet: \"{tweet}\"\n",
    "    Existing Event Summary: \"{existing_summary or 'None'}\"\n",
    "\n",
    "    Respond only in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGBackend1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"event_type\": \"War\",\n",
      "    \"location\": [\"India\", \"Pakistan\"],\n",
      "    \"people_killed\": \"Unknown\",\n",
      "    \"people_trapped\": \"Unknown\",\n",
      "    \"infrastructure_damage\": \"Unknown\",\n",
      "    \"other_details\": \"Conflict initiated due to Pakistan hitting different cities in India\",\n",
      "    \"summary\": \"War has erupted between India and Pakistan, instigated by Pakistan's attacks on various Indian cities.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "result found: {'id': '7bff9c0c-f1cd-4b56-912b-3a5be16cab61', 'distance': 0.7950828075408936}\n",
      "First event result: {'action': 'added', 'id': 'a5ed4dcc-f114-49d4-aee7-4a38acf6ff4d'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Submit a new event\n",
    "text = \"India and pakistan war is started again due to pakistan hitting india different cities\"\n",
    "\n",
    "gpt_response = call_gpt_extractor(text)\n",
    "print(gpt_response)\n",
    "result_1 = rag.process_event(gpt_response)\n",
    "print(\"First event result:\", result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisasterExtraction(tweet):\n",
    "    rag = RAGBackend1()\n",
    "    info_category = binary_classifier(tweet)\n",
    "    if info_category==\"non-informative\":\n",
    "        return \"This tweet doesn't contain any disaster related information.\"\n",
    "    humanitarian_category = humanitarianClassifier(tweet)\n",
    "    tweet+= f\", Category: {humanitarian_category}\"\n",
    "    gpt_response = call_gpt_extractor(tweet)\n",
    "    result = rag.process_event(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
