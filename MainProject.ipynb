{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf-gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"key\"\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_metadata = {}\n",
    "# Mapping between FAISS numeric ID and real UUID event_id\n",
    "faiss_id_to_event_id = {}\n",
    "metadata_file_path = \"event_store/events_metadata.json\"\n",
    "idmap_file_path = \"event_store/faiss_id_map.json\"\n",
    "os.makedirs(os.path.dirname(metadata_file_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save metadata\n",
    "def sync_event_metadata_to_json(metadata):\n",
    "    with open(metadata_file_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    with open(idmap_file_path, \"w\") as f:\n",
    "        json.dump(faiss_id_to_event_id, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_metadata_and_faiss():\n",
    "    global event_metadata, faiss_index, faiss_id_to_event_id\n",
    "    if os.path.exists(metadata_file_path):\n",
    "        try:\n",
    "            with open(metadata_file_path, \"r\") as f:\n",
    "                content = f.read().strip()\n",
    "                event_metadata = json.loads(content) if content else {}\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è events_metadata.json is empty or corrupted. Initializing new store.\")\n",
    "            event_metadata = {}\n",
    "\n",
    "        if os.path.exists(idmap_file_path):\n",
    "            try:\n",
    "                with open(idmap_file_path, \"r\") as f:\n",
    "                    content = f.read().strip()\n",
    "                    faiss_id_to_event_id = json.loads(content) if content else {}\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"‚ö†Ô∏è faiss_id_map.json is empty or corrupted. Initializing new map.\")\n",
    "                faiss_id_to_event_id = {}\n",
    "\n",
    "        rebuild_faiss_index()\n",
    "        print(f\"\\u2705 Loaded {len(event_metadata)} events from disk.\")\n",
    "    else:\n",
    "        event_metadata = {}\n",
    "        faiss_id_to_event_id = {}\n",
    "        faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))\n",
    "        print(\"\\U0001f195 No metadata found. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuilding fiass index\n",
    "def get_faiss_compatible_id(event_id):\n",
    "    return hash(event_id) % (2**63 - 1) \n",
    "\n",
    "def rebuild_faiss_index():\n",
    "    global faiss_index\n",
    "    faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))\n",
    "    for event_id, data in event_metadata.items():\n",
    "        embedding = embedding_model.encode([data[\"summary\"]])[0]\n",
    "        event_int_id = get_faiss_compatible_id(event_id)\n",
    "        faiss_index.add_with_ids(\n",
    "            np.array([embedding], dtype=np.float32),\n",
    "            np.array([event_int_id], dtype=np.int64)\n",
    "        )\n",
    "        faiss_id_to_event_id[event_int_id] = event_id\n",
    "    print(f\"\\U0001f501 FAISS rebuilt with {len(event_metadata)} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    return embedding_model.encode([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_extractor(tweet, existing_summary):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in extracting structured information from tweets about disasters.\n",
    "    Given the tweet and existing event summary, return a JSON with important details about the event.\n",
    "    The JSON may include the following fields:\n",
    "    - event_type\n",
    "    - location\n",
    "    - people_killed\n",
    "    - people_trapped\n",
    "    - infrastructure_damage\n",
    "    - any other details you find relevant\n",
    "    - summary (updated, more detailed)\n",
    "\n",
    "    Tweet: \"{tweet}\"\n",
    "    Existing Event Summary: \"{existing_summary or 'None'}\"\n",
    "\n",
    "    Respond only in JSON format.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_event(tweet_text, threshold=0.75):\n",
    "    if faiss_index.ntotal == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    query_embedding = get_embedding(tweet_text).reshape(1, -1)\n",
    "    D, I = faiss_index.search(query_embedding, k=1)\n",
    "\n",
    "    best_faiss_id = I[0][0]\n",
    "    best_distance = D[0][0]\n",
    "    best_score = 1 - best_distance / 2\n",
    "\n",
    "    event_id = faiss_id_to_event_id.get(str(best_faiss_id))\n",
    "    if event_id and best_score >= threshold:\n",
    "        return event_id, event_metadata[event_id], best_faiss_id\n",
    "\n",
    "    return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet_text):\n",
    "    matched_id, matched_event, _ = find_matching_event(tweet_text)\n",
    "    existing_summary = matched_event[\"summary\"] if matched_event else None\n",
    "\n",
    "    gpt_output = call_gpt_extractor(tweet_text, existing_summary)\n",
    "\n",
    "    try:\n",
    "        parsed_data = json.loads(gpt_output)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Invalid JSON from GPT\", \"raw_output\": gpt_output}\n",
    "\n",
    "    updated_summary = parsed_data[\"summary\"]\n",
    "    embedding = get_embedding(updated_summary)\n",
    "\n",
    "    if matched_id:\n",
    "        event_metadata[matched_id] = {\n",
    "            \"summary\": updated_summary,\n",
    "            \"structured_data\": parsed_data,\n",
    "            \"last_tweet\": tweet_text\n",
    "        }\n",
    "    else:\n",
    "        new_event_id = str(uuid.uuid4())\n",
    "        event_metadata[new_event_id] = {\n",
    "            \"summary\": updated_summary,\n",
    "            \"structured_data\": parsed_data,\n",
    "            \"last_tweet\": tweet_text\n",
    "        }\n",
    "        matched_id = new_event_id\n",
    "\n",
    "    # Rebuild FAISS index and sync metadata\n",
    "    sync_event_metadata_to_json(event_metadata)\n",
    "    rebuild_faiss_index()\n",
    "\n",
    "    return {\"event_id\": matched_id, \"data\": parsed_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ FAISS rebuilt with 0 vectors.\n",
      "‚úÖ Loaded 0 events from disk.\n"
     ]
    }
   ],
   "source": [
    "load_event_metadata_and_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_1 = \"A 6.8 magnitude earthquake strikes Tokyo, Japan. 18 people reported dead, several buildings have collapsed. Rescue operations underway.\"\n",
    "tweet_2 = \"Cyclone Mahina lashes Bangladesh coast. Winds up to 150 km/h. 40 fishermen missing and 200 homes destroyed.\"\n",
    "tweet_3 = \"Massive wildfire near Los Angeles spreads to 50,000 acres. Over 70 houses burned, 15 people injured, evacuation ongoing.\"\n",
    "tweet_4 = \"Explosion at chemical plant in Houston. Toxic smoke spreading, 3 confirmed dead, dozens hospitalized. Emergency declared.\"\n",
    "tweet_5 = \"Heavy rains cause flooding in Venice. 10,000 evacuated, historic sites submerged, damage expected to be in millions.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ FAISS rebuilt with 1 vectors.\n",
      "{'event_id': 'd4a0daaa-79cf-4054-93fc-35dcac44fcb1', 'data': {'event_type': 'Earthquake', 'location': 'Tokyo, Japan', 'people_killed': 18, 'infrastructure_damage': 'Several buildings have collapsed', 'rescue_operations': 'Underway', 'summary': 'A 6.8 magnitude earthquake has struck Tokyo, Japan. 18 people have been reported dead and several buildings have collapsed. Rescue operations are currently underway.'}}\n",
      "üîÅ FAISS rebuilt with 2 vectors.\n",
      "{'event_id': '92aa4802-a09b-42d8-a260-f3f14055cc3a', 'data': {'event_type': 'Cyclone', 'location': 'Bangladesh coast', 'people_missing': 40, 'infrastructure_damage': '200 homes destroyed', 'wind_speed': 'up to 150 km/h', 'summary': 'Cyclone Mahina has hit the Bangladesh coast with winds up to 150 km/h. 40 fishermen are reported missing and 200 homes have been destroyed.'}}\n",
      "üîÅ FAISS rebuilt with 3 vectors.\n",
      "{'event_id': '7db33f71-3849-44ce-85e7-44887a79aacf', 'data': {'event_type': 'Wildfire', 'location': 'Los Angeles', 'people_injured': 15, 'infrastructure_damage': 'Over 70 houses burned', 'evacuation': 'ongoing', 'area_affected': '50,000 acres', 'summary': 'A massive wildfire near Los Angeles has spread to 50,000 acres. Over 70 houses have been burned and 15 people have been injured. Evacuation is currently ongoing.'}}\n",
      "üîÅ FAISS rebuilt with 4 vectors.\n",
      "{'event_id': '23bd072e-6363-4893-b074-19e91e442b93', 'data': {'event_type': 'Chemical Plant Explosion', 'location': 'Houston', 'people_killed': 3, 'people_hospitalized': 'dozens', 'infrastructure_damage': 'Chemical plant', 'additional_details': 'Toxic smoke spreading, emergency declared', 'summary': 'An explosion occurred at a chemical plant in Houston. The incident has resulted in toxic smoke spreading across the area, leading to the declaration of an emergency. Three people have been confirmed dead, with dozens more hospitalized.'}}\n",
      "üîÅ FAISS rebuilt with 5 vectors.\n",
      "{'event_id': '7aeec699-b10f-4e84-a1b7-e5b81f8c8537', 'data': {'event_type': 'Flood', 'location': 'Venice', 'people_evacuated': 10000, 'historic_sites_affected': True, 'infrastructure_damage': 'Expected to be in millions', 'summary': 'Heavy rains have caused severe flooding in Venice, leading to the evacuation of 10,000 people. Historic sites are submerged and the damage is expected to be in the millions.'}}\n"
     ]
    }
   ],
   "source": [
    "for tweet in [tweet_1, tweet_2, tweet_3, tweet_4, tweet_5]:\n",
    "    result = process_tweet(tweet)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
